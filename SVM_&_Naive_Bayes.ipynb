{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "Ans:- Support Vector Machine (SVM)\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It's a powerful and popular algorithm in machine learning.\n",
        "\n",
        "How SVM Works\n",
        "\n",
        "* Objective: SVM aims to find the best hyperplane (decision boundary) that separates the data into different classes.\n",
        "* Hyperplane: A hyperplane is a line (in 2D) or a plane (in 3D) that separates the data.\n",
        "* Support Vectors: The data points closest to the hyperplane are called support vectors. These points are crucial in defining the hyperplane.\n",
        "* Margin: The distance between the hyperplane and the support vectors is called the margin. SVM aims to maximize this margin.\n",
        "* Kernel Trick: SVM uses the kernel trick to transform the data into a higher-dimensional space, making it easier to find a hyperplane.\n",
        "\n",
        "Key Characteristics\n",
        "\n",
        "- Robust to noise: SVM is robust to noise and outliers.\n",
        "- Effective in high-dimensional spaces: SVM works well in high-dimensional spaces.\n",
        "- Memory-intensive: SVM can be memory-intensive for large datasets.\n",
        "\n",
        "Types of SVM\n",
        "\n",
        "- Linear SVM: Used for linearly separable data.\n",
        "- Non-Linear SVM: Used for non-linearly separable data (uses kernel trick).\n",
        "\n",
        "Common Kernels\n",
        "\n",
        "- Linear Kernel: Used for linearly separable data.\n",
        "- Polynomial Kernel: Used for polynomial relationships.\n",
        "- Radial Basis Function (RBF) Kernel: Used for non-linear relationships.\n",
        "\n",
        "SVM is widely used in various applications, including image classification, text classification, and bioinformatics.\n",
        "\n",
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "Ans:- Hard Margin SVM vs Soft Margin SVM\n",
        "\n",
        "Hard Margin SVM and Soft Margin SVM are two types of SVM approaches used for classification tasks.\n",
        "\n",
        "Hard Margin SVM\n",
        "\n",
        "- Assumes linearly separable data: Hard Margin SVM assumes that the data is linearly separable, meaning it can be separated by a hyperplane without any errors.\n",
        "- No misclassifications allowed: Hard Margin SVM doesn't allow any misclassifications, meaning all data points must be on the correct side of the hyperplane.\n",
        "- Sensitive to noise and outliers: Hard Margin SVM is sensitive to noise and outliers, as a single misclassified point can significantly affect the hyperplane.\n",
        "- Prone to overfitting: Hard Margin SVM can be prone to overfitting, especially when the data is noisy or has outliers.\n",
        "\n",
        "Soft Margin SVM\n",
        "\n",
        "- Allows misclassifications: Soft Margin SVM allows some misclassifications, meaning some data points can be on the wrong side of the hyperplane.\n",
        "- Introduces slack variables: Soft Margin SVM introduces slack variables to relax the constraints, allowing some data points to be misclassified.\n",
        "- More robust to noise and outliers: Soft Margin SVM is more robust to noise and outliers, as it can tolerate some misclassifications.\n",
        "- Less prone to overfitting: Soft Margin SVM is less prone to overfitting, as it allows some misclassifications and is more robust to noise.\n",
        "\n",
        "Key differences\n",
        "\n",
        "- Handling misclassifications: Hard Margin SVM doesn't allow misclassifications, while Soft Margin SVM allows some misclassifications.\n",
        "- Robustness to noise: Soft Margin SVM is more robust to noise and outliers compared to Hard Margin SVM.\n",
        "- Overfitting: Soft Margin SVM is less prone to overfitting compared to Hard Margin SVM.\n",
        "\n",
        "In practice, Soft Margin SVM is more commonly used, as it can handle noisy and non-linearly separable data.\n",
        "\n",
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "Ans:- Kernel Trick in SVM\n",
        "\n",
        "The Kernel Trick is a technique used in SVM to transform the original data into a higher-dimensional space, making it easier to find a hyperplane that separates the data. This is done without explicitly computing the transformation, which can be computationally expensive.\n",
        "\n",
        "How the Kernel Trick works\n",
        "\n",
        "* Compute dot products: Instead of transforming the data, SVM computes the dot product of the data points in the original space.\n",
        "* Use a kernel function: A kernel function is used to compute the dot product in the higher-dimensional space.\n",
        "* Replace dot products: The kernel function replaces the dot product computation, allowing SVM to work in the higher-dimensional space without explicit transformation.\n",
        "\n",
        "Example: Radial Basis Function (RBF) Kernel\n",
        "\n",
        "The RBF kernel is a popular kernel used in SVM. It's defined as:\n",
        "\n",
        "K(x, y) = exp(-γ ||x - y||^2)\n",
        "\n",
        "where x and y are data points, γ is a hyperparameter, and ||.|| is the Euclidean norm.\n",
        "\n",
        "Use case: Non-linearly separable data\n",
        "\n",
        "The RBF kernel is useful when the data is non-linearly separable. It maps the data into a higher-dimensional space, making it easier to find a hyperplane that separates the data.\n",
        "\n",
        "For example, consider a dataset with two features (x1, x2) and two classes. The data is non-linearly separable in the original space, but the RBF kernel can transform it into a higher-dimensional space where it's linearly separable.\n",
        "\n",
        "The RBF kernel is widely used in image classification, text classification, and other applications where the data is non-linearly separable.\n",
        "\n",
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "Ans:- Naïve Bayes Classifier\n",
        "\n",
        "A Naïve Bayes Classifier is a probabilistic machine learning model used for classification tasks. It's based on Bayes' theorem and assumes independence between features, which is why it's called \"naïve\".\n",
        "\n",
        "How Naïve Bayes works\n",
        "\n",
        "* Bayes' theorem: The classifier uses Bayes' theorem to update the probability of a class given the features.\n",
        "* Independence assumption: The classifier assumes that the features are independent, meaning the presence or absence of one feature doesn't affect the others.\n",
        "* Calculate probabilities: The classifier calculates the probability of each class given the features and chooses the class with the highest probability.\n",
        "\n",
        "Why is it called \"naïve\"?\n",
        "\n",
        "It's called \"naïve\" because of the independence assumption, which is often unrealistic in real-world data. In most cases, features are not independent, and there are correlations between them. Despite this, Naïve Bayes often performs surprisingly well, especially with large datasets.\n",
        "\n",
        "Key characteristics\n",
        "\n",
        "- Simple and efficient: Naïve Bayes is a simple and efficient algorithm, making it suitable for large datasets.\n",
        "- Probabilistic: The classifier provides probabilities for each class, allowing for uncertainty estimation.\n",
        "- Handles high-dimensional data: Naïve Bayes can handle high-dimensional data, making it suitable for text classification and other applications.\n",
        "\n",
        "Common applications\n",
        "\n",
        "- Text classification: Naïve Bayes is widely used in text classification tasks, such as spam filtering and sentiment analysis.\n",
        "- Image classification: It's also used in image classification tasks, such as object detection and image categorization.\n",
        "\n",
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.When would you use each one?\n",
        "Ans:- Naïve Bayes Variants\n",
        "\n",
        "There are three main variants of Naïve Bayes classifiers, each suited for different types of data:\n",
        "\n",
        "* Gaussian Naïve Bayes- Assumes continuous features: Gaussian Naïve Bayes assumes that the features are continuous and follow a Gaussian (normal) distribution.\n",
        "- Use case: Use Gaussian Naïve Bayes when dealing with continuous features, such as image pixel values or sensor readings.\n",
        "\n",
        "* Multinomial Naïve Bayes- Assumes count-based features: Multinomial Naïve Bayes assumes that the features are count-based, such as word frequencies in text classification.\n",
        "- Use case: Use Multinomial Naïve Bayes when dealing with text classification, sentiment analysis, or other count-based features.\n",
        "\n",
        "* Bernoulli Naïve Bayes- Assumes binary features: Bernoulli Naïve Bayes assumes that the features are binary (0/1, yes/no, etc.).\n",
        "- Use case: Use Bernoulli Naïve Bayes when dealing with binary features, such as presence/absence of words in text or binary sensor data.\n",
        "\n",
        "Choosing the right variant\n",
        "\n",
        "- Check feature type: Choose the variant based on the type of features you're working with (continuous, count-based, or binary).\n",
        "- Data distribution: Consider the distribution of your data and choose the variant that best matches it.\n",
        "\n",
        "Here's a rough guide:\n",
        "\n",
        "| Feature Type | Naïve Bayes Variant |\n",
        "| ------------ | -------------------- |\n",
        "| Continuous | Gaussian |\n",
        "| Count-based | Multinomial |\n",
        "| Binary | Bernoulli |\n",
        "\n",
        "\n",
        "Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n"
      ],
      "metadata": {
        "id": "AIgmDVCaHkh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier with a linear kernel\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print the support vectors\n",
        "print(\"Support Vectors:\")\n",
        "print(svm.support_vectors_)\n",
        "\n",
        "# Print the number of support vectors\n",
        "print(f\"Number of Support Vectors: {len(svm.support_vectors_)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN51XhYRPQdT",
        "outputId": "2965303a-dccd-479f-ca17-989f5dfc42cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n",
            "Number of Support Vectors: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "Mfl8FZsHPlLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1nwolYjPuBL",
        "outputId": "e78b6012-6445-4d0e-daf5-d1150b58f1f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy."
      ],
      "metadata": {
        "id": "omW_LVQxQLoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.01, 0.1, 1, 10],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and train an SVM Classifier\n",
        "best_svm = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "y_pred = best_svm.predict(X_test)\n",
        "\n",
        "# Print the best hyperparameters and accuracy\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv84lfg9QQkw",
        "outputId": "daab0ed0-edfb-4d81-d0f2-80a0b0d43bc3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "Accuracy: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions."
      ],
      "metadata": {
        "id": "b2BXCLoJQyF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the 20 Newsgroups dataset (binary classification, e.g., 'alt.atheism' vs 'soc.religion.christian')\n",
        "categories = ['alt.atheism', 'soc.religion.christian']\n",
        "newsgroups = fetch_20newsgroups(categories=categories)\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Multinomial Naïve Bayes Classifier\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred_proba = mnb.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSf0vhCGQ27t",
        "outputId": "73ee71d7-c45e-4d34-a380-66d3623191b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution"
      ],
      "metadata": {
        "id": "ojQRvXwNRMgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Ans:-  Approach to Email Classification\n",
        "\n",
        "Preprocessing the Data1. Text Vectorization: Use TF-IDF (Term Frequency-Inverse Document Frequency) to convert text data into numerical vectors. This will help capture the importance of words in the email content.\n",
        "* Handling Missing Data: Use imputation techniques like replacing missing values with mean/median or using a K-Nearest Neighbors (KNN) imputer. Alternatively, consider using models that can handle missing data, like decision trees or random forests.\n",
        "* Data Cleaning: Remove stop words, punctuation, and special characters. Apply stemming or lemmatization to reduce words to their base form.\n",
        "\n",
        "Model Selection1. Naïve Bayes: A good starting point for text classification tasks, especially with TF-IDF vectorization. It's simple, efficient, and often performs well.\n",
        "* SVM: A strong contender, especially with class imbalance. However, it may require more tuning and computational resources.\n",
        "\n",
        "Given the text nature of the data and potential class imbalance, I would start with Multinomial Naïve Bayes and compare its performance with SVM.\n",
        "\n",
        "Addressing Class Imbalance1. Class Weighting: Use class weighting techniques, like assigning higher weights to the minority class (Spam), to help the model focus on it.\n",
        "* Oversampling/Undersampling: Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) or random undersampling to balance the classes.\n",
        "* Anomaly Detection: Consider framing the problem as anomaly detection, where Spam emails are treated as anomalies.\n",
        "\n",
        "Evaluation Metrics1. Precision: Important to ensure that legitimate emails aren't incorrectly classified as Spam.\n",
        "* Recall: Crucial to catch as many Spam emails as possible.\n",
        "* F1-score: A balanced measure of precision and recall.\n",
        "* AUC-ROC: Evaluate the model's ability to distinguish between classes.\n",
        "\n",
        "Business Impact1. Improved Email Experience: Accurate Spam filtering will improve the overall email experience for users, reducing clutter and increasing productivity.\n",
        "* Reduced False Positives: Minimizing false positives (legitimate emails marked as Spam) will ensure important emails aren't missed.\n",
        "* Increased Security: Effective Spam filtering will reduce phishing and malware risks, protecting users and the company.\n",
        "* Cost Savings: Automating Spam filtering will reduce manual effort and costs associated with email management.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SKOWWKVgRhar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Load email data\n",
        "# Replace with your actual email data and labels\n",
        "emails = [\n",
        "    \"This is a legitimate email about a meeting.\",\n",
        "    \"Buy now and get a free gift! Limited time offer.\",\n",
        "    \"Meeting scheduled for tomorrow at 10 AM.\",\n",
        "    \"Click here to claim your prize!\",\n",
        "    \"Important update regarding your account.\",\n",
        "    \"Spam email with malicious link.\",\n",
        "    \"Just a friendly reminder.\",\n",
        "    \"Earn money fast!\",\n",
        "    \"Project status update.\",\n",
        "    \"Conferences and workshops.\"\n",
        "]\n",
        "labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 0]  # 0: Not Spam, 1: Spam\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(emails, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "# Train a Multinomial Naïve Bayes Classifier\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = mnb.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1-score: {f1_score(y_test, y_pred, average='weighted'):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xD7AnHkSZPa",
        "outputId": "2a57f0dc-94a4-4b6b-82b7-a92a9c3c8310"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67         1\n",
            "           1       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n",
            "F1-score: 0.33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}